{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjmXmszUA5bN"
   },
   "source": [
    "# Working with numerical features\n",
    "\n",
    "We have to prepare our data to work with ML algorithms. In the case of numerical values we have some methods that we should apply before we start working with ML algorithms. Some of those methosts are:\n",
    "\n",
    "    Imputation\n",
    "    Handling Outliers\n",
    "    Feature Scaling\n",
    "    Feature Transformation\n",
    "    Binning\n",
    "    Log Transform\n",
    "   \n",
    "\n",
    "We saw how to work with outliers, and null values, and the techniques for imputation of NaN values. In this lesson we are going to focus on scaling, bining and log transformation.\n",
    "\n",
    "\n",
    "We discussed previously that the scale of the features is an important consideration when building machine learning models. Briefly:\n",
    "Feature magnitude matters because:\n",
    "\n",
    "    The regression coefficients of linear models are directly influenced by the scale of the variable.\n",
    "    Variables with bigger magnitude / larger value range dominate over those with smaller magnitude / value range\n",
    "    Gradient descent converges faster when features are on similar scales\n",
    "    Feature scaling helps decrease the time to find support vectors for SVMs\n",
    "    Euclidean distances are sensitive to feature magnitude.\n",
    "    Some algorithms, like PCA require the features to be centered at 0.\n",
    "\n",
    "The machine learning models affected by the feature scale are:\n",
    "\n",
    "    Linear and Logistic Regression\n",
    "    Neural Networks\n",
    "    Support Vector Machines\n",
    "    KNN\n",
    "    K-means clustering\n",
    "    Linear Discriminant Analysis (LDA)\n",
    "    Principal Component Analysis (PCA)\n",
    "\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "Feature scaling refers to the methods or techniques used to normalize the range of independent variables in our data, or in other words, the methods to set the feature value range within a similar scale. Feature scaling is generally the last step in the data preprocessing pipeline, performed just before training the machine learning algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iNRSx2dfA5bO"
   },
   "source": [
    "## Feature Scaling: Z-Score Standardization and Min-Max Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0ORIQYJA5bP"
   },
   "source": [
    "- [About standardization](#About-standardization)\n",
    "- [About Min-Max scaling / \"normalization\"](#About-Min-Max-scaling-normalization)\n",
    "- [Standardization or Min-Max scaling?](#Standardization-or-Min-Max-scaling?)\n",
    "- [Standardizing and normalizing - how it can be done using scikit-learn](#Standardizing-and-normalizing---how-it-can-be-done-using-scikit-learn)\n",
    "- [Bottom-up approaches](#Bottom-up-approaches)\n",
    "- [The effect of standardization on PCA in a pattern classification task](#The-effect-of-standardization-on-PCA-in-a-pattern-classification-task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ti17xhGNA5bP"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s8eIevUaA5bQ"
   },
   "source": [
    "### About standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4iuLMzyIA5bQ"
   },
   "source": [
    "The result of **standardization** (or **Z-score normalization**) is that the features will be rescaled so that they'll have the properties of a standard normal distribution with   \n",
    "\n",
    "$\\mu = 0$ and $\\sigma = 1$\n",
    "\n",
    "where $\\mu$ is the mean (average) and $\\sigma$ is the standard deviation from the mean; standard scores (also called ***z*** scores) of the samples are calculated as follows:\n",
    "\n",
    "\\begin{equation} z = \\frac{x - \\mu}{\\sigma}\\end{equation} \n",
    "\n",
    "Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. Intuitively, we can think of gradient descent as a prominent example\n",
    "(an optimization algorithm often used in logistic regression, SVMs, perceptrons, neural networks etc.); with features being on different scales, certain weights may update faster than others since the feature values $x_j$ play a role in the weight updates\n",
    "\n",
    "$$\\Delta w_j = - \\eta \\frac{\\partial J}{\\partial w_j} = \\eta \\sum_i (t^{(i)} - o^{(i)})x^{(i)}_{j},$$\n",
    "\n",
    "so that \n",
    "\n",
    "$$w_j := w_j + \\Delta w_j,$$\n",
    "where $\\eta$ is the learning rate, $t$ the target class label, and $o$ the actual output.\n",
    "Other intuitive examples include K-Nearest Neighbor algorithms and clustering algorithms that use, for example, Euclidean distance measures -- in fact, tree-based classifier are probably the only classifiers where feature scaling doesn't make a difference.\n",
    "\n",
    "\n",
    "\n",
    "To quote from the [`scikit-learn`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) documentation:\n",
    "\n",
    "*\"Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFSDfRzDA5bR"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5A0INeBA5bR"
   },
   "source": [
    "<a id='About-Min-Max-scaling-normalization'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nI1hkCE_A5bS"
   },
   "source": [
    "### About Min-Max scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9Jf3rSTA5bS"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVm48vnkA5bT"
   },
   "source": [
    "An alternative approach to Z-score normalization (or standardization) is the so-called **Min-Max scaling** (often also simply called \"normalization\" - a common cause for ambiguities).  \n",
    "In this approach, the data is scaled to a fixed range - usually 0 to 1.  \n",
    "The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n",
    "\n",
    "A Min-Max scaling is typically done via the following equation:\n",
    "\n",
    "\\begin{equation} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QCXeBnBA5bT"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jxrqn_WEA5bU"
   },
   "source": [
    "### Z-score standardization or Min-Max scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfQWCindA5bU"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pcYyZgW_A5bV"
   },
   "source": [
    "*\"Standardization or Min-Max scaling?\"* - There is no obvious answer to this question: it really depends on the application. \n",
    "\n",
    "For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance (depending on the question and if the PCA computes the components via the correlation matrix instead of the covariance matrix; [but more about PCA in my previous article](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)).\n",
    "\n",
    "However, this doesn't mean that Min-Max scaling is not useful at all! A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Also, typical neural network algorithm require data that on a 0-1 scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzrirBRmA5bV"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kX1ITX9yA5bW"
   },
   "source": [
    "## Standardizing and normalizing - how it can be done using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQPknixmA5bW"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1B5R04aA5bW"
   },
   "source": [
    "Of course, we could make use of NumPy's vectorization capabilities to calculate the z-scores for standardization and to normalize the data using the equations that were mentioned in the previous sections. However, there is an even more convenient approach using the preprocessing module from one of Python's open-source machine learning library [scikit-learn](http://scikit-learn.org )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWEN_OL5A5bX"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOakFslHA5bX"
   },
   "source": [
    "For the following examples and discussion, we will have a look at the free \"Wine\" Dataset that is deposited on the UCI machine learning repository  \n",
    "(http://archive.ics.uci.edu/ml/datasets/Wine).\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=\"1\">\n",
    "**Reference:**  \n",
    "Forina, M. et al, PARVUS - An Extendible Package for Data\n",
    "Exploration, Classification and Correlation. Institute of Pharmaceutical\n",
    "and Food Analysis and Technologies, Via Brigata Salerno, \n",
    "16147 Genoa, Italy.\n",
    "\n",
    "Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91cm_5heA5bY"
   },
   "source": [
    "The Wine dataset consists of 3 different classes where each row correspond to a particular wine sample.\n",
    "\n",
    "The class labels (1, 2, 3) are listed in the first column, and the columns 2-14 correspond to 13 different attributes (features):\n",
    "\n",
    "1) Alcohol  \n",
    "2) Malic acid  \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oF1VqrLA5bY"
   },
   "source": [
    "#### Loading the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "S6edsgZMA5bZ",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ead5438b-7990-4a8f-9ac2-5e1c89c3fbfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.io.parsers.read_csv(\\n    'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', \\n     header=None,\\n     usecols=[0,1,2]\\n    )\\n\\ndf.columns=['Class label', 'Alcohol', 'Malic acid']\\n\\ndf.head()\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.io.parsers.read_csv(\n",
    "    'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', \n",
    "     header=None,\n",
    "     usecols=[0,1,2]\n",
    "    )\n",
    "\n",
    "df.columns=['Class label', 'Alcohol', 'Malic acid']\n",
    "\n",
    "df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SHQKwf7A5bc"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxm6pRQBA5bd"
   },
   "source": [
    "As we can see in the table above, the features **Alcohol** (percent/volumne) and **Malic acid** (g/l) are measured on different scales, so that ***Feature Scaling*** is necessary important prior to any comparison or combination of these data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KwkhPa5A5bd"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvTYGjOrA5be"
   },
   "source": [
    "#### Standardization and Min-Max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "fVbIgf2PA5be",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "v93uyjO_A5bh",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6287a137-d1a3-4c99-d61d-fb5fb5cfb4d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Mean after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\\n      .format( , ))\\nprint('\\nStandard deviation after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\\n      .format( , ))\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print('Mean after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format( , ))\n",
    "print('\\nStandard deviation after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format( , ))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "6ytjA_lHA5bj",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "fc867db1-5119-49e3-e5b7-130f5376445b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Min-value after min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\\n      .format( , ))\\nprint('\\nMax-value after min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\\n      .format( , ))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print('Min-value after min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format( , ))\n",
    "print('\\nMax-value after min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format( , ))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFkDQES_A5bl"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFYVfTZNA5bm"
   },
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "77K__G6WA5bm",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "Qfm1-VqJA5br",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "9271d145-5550-43ef-fab2-b2c9180ab8cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom matplotlib import pyplot as plt\\n\\ndef plot():\\n    plt.figure(figsize=(8,6))\\n\\n    plt.scatter(df['Alcohol'], df['Malic acid'], \\n            color='green', label='input scale', alpha=0.5)\\n\\n    plt.scatter(np_std[:,0], np_std[:,1], color='red', \\n            label='Standardized [$N  (\\\\mu=0, \\\\; \\\\sigma=1)$]', alpha=0.3)\\n\\n    plt.scatter(np_minmax[:,0], np_minmax[:,1], \\n            color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3)\\n\\n    plt.title('Alcohol and Malic Acid content of the wine dataset')\\n    plt.xlabel('Alcohol')\\n    plt.ylabel('Malic Acid')\\n    \\n    ############################## COMPLETAR\\n    ############################## COMPLETAR\\n    \\n    plt.tight_layout() # te ajusta para que se vea bien\\n\\nplot() #¡la acabo de definir yo!\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot():\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.scatter(df['Alcohol'], df['Malic acid'], \n",
    "            color='green', label='input scale', alpha=0.5)\n",
    "\n",
    "    plt.scatter(np_std[:,0], np_std[:,1], color='red', \n",
    "            label='Standardized [$N  (\\mu=0, \\; \\sigma=1)$]', alpha=0.3)\n",
    "\n",
    "    plt.scatter(np_minmax[:,0], np_minmax[:,1], \n",
    "            color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3)\n",
    "\n",
    "    plt.title('Alcohol and Malic Acid content of the wine dataset')\n",
    "    plt.xlabel('Alcohol')\n",
    "    plt.ylabel('Malic Acid')\n",
    "    \n",
    "    ############################## COMPLETAR\n",
    "    ############################## COMPLETAR\n",
    "    \n",
    "    plt.tight_layout() # te ajusta para que se vea bien\n",
    "\n",
    "plot() #¡la acabo de definir yo!\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oG7IESkRA5bu"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EbYAU0ZoA5bu"
   },
   "source": [
    "The plot above includes the wine datapoints on all three different scales: the input scale where the alcohol content was measured in volume-percent (green), the standardized features (red), and the normalized features (blue).\n",
    "In the following plot, we will zoom in into the three different axis-scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordatorio de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en numpy un array UNIDIMENSIONAL tiene de dimensiones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un numpy un array 2D tiene de dimensiones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡Importante!\n",
    "# No es lo mismo un array UNIDIMENSIONAL que uno en 2D con solo una fila o solo una columna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un array UNIDIMENSIONAL transpuesto se queda igual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un array 2D con una fila o una columna sí se transpone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos el método zip de Python\n",
    "# zip itera sobre tuplas y crea tuplas nuevas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si zip tiene una sola tupla de argumento\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguimos con el tratamiento de datos numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2tqyHUpA5bv"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "aSSt2gUEA5bv",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "13379129-5f89-43fc-ebd1-301006a22971"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nfig, ax = plt.subplots(3, figsize=(6,14))\\n\\nfor a,d,l in zip(range(len(ax)),\\n               (df[['Alcohol', 'Malic acid']].values, np_std, np_minmax), # values devuelve los valores en tipo numpy\\n               ('Input scale', \\n                'Standardized [$N  (\\\\mu=0, \\\\; \\\\sigma=1)$]', \\n                'min-max scaled [min=0, max=1]')\\n                ):\\n    # a es 0, 1, 2\\n    # d es df[['Alcohol', 'Malic acid']].values, np_std, np_minmax \\n    # l es 'Input scale', 'Standardized [$N  (\\\\mu=0, \\\\; \\\\sigma=1)$]', 'min-max scaled [min=0, max=1]'\\n    \\n    for i,c in zip(range(1,4), ('red', 'blue', 'green')):\\n        ax[a].scatter(d[df['Class label'].values == i, 0], \\n                  d[df['Class label'].values == i, 1],\\n                  alpha=0.5,\\n                  color=c,\\n                  label='Class %s' %i\\n                  )\\n        # i es 1, 2, 3\\n        # c es 'red', 'blue', 'green'\\n        \\n    ax[a].set_title(l)\\n    ax[a].set_xlabel('Alcohol')\\n    ax[a].set_ylabel('Malic Acid')\\n    ax[a].legend(loc='upper left')\\n    ax[a].grid()\\n    \\n################ COMPLETAR\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(6,14))\n",
    "\n",
    "for a,d,l in zip(range(len(ax)),\n",
    "               (df[['Alcohol', 'Malic acid']].values, np_std, np_minmax), # values devuelve los valores en tipo numpy\n",
    "               ('Input scale', \n",
    "                'Standardized [$N  (\\mu=0, \\; \\sigma=1)$]', \n",
    "                'min-max scaled [min=0, max=1]')\n",
    "                ):\n",
    "    # a es 0, 1, 2\n",
    "    # d es df[['Alcohol', 'Malic acid']].values, np_std, np_minmax \n",
    "    # l es 'Input scale', 'Standardized [$N  (\\mu=0, \\; \\sigma=1)$]', 'min-max scaled [min=0, max=1]'\n",
    "    \n",
    "    for i,c in zip(range(1,4), ('red', 'blue', 'green')):\n",
    "        ax[a].scatter(d[df['Class label'].values == i, 0], \n",
    "                  d[df['Class label'].values == i, 1],\n",
    "                  alpha=0.5,\n",
    "                  color=c,\n",
    "                  label='Class %s' %i\n",
    "                  )\n",
    "        # i es 1, 2, 3\n",
    "        # c es 'red', 'blue', 'green'\n",
    "        \n",
    "    ax[a].set_title(l)\n",
    "    ax[a].set_xlabel('Alcohol')\n",
    "    ax[a].set_ylabel('Malic Acid')\n",
    "    ax[a].legend(loc='upper left')\n",
    "    ax[a].grid()\n",
    "    \n",
    "################ COMPLETAR\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igHy0_aCA5bx"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicar values a un DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIY4clVRA5bx"
   },
   "source": [
    "## Bottom-up approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqiB5GbhA5by"
   },
   "source": [
    "Of course, we can also code the equations for standardization and 0-1 Min-Max scaling \"manually\". However, the scikit-learn methods are still useful if you are working with test and training data sets and want to scale them equally.\n",
    "\n",
    "E.g., \n",
    "<pre>\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = std_scale.transform(X_train)\n",
    "X_test = std_scale.transform(X_test)\n",
    "</pre>\n",
    "\n",
    "Below, we will perform the calculations using \"pure\" Python code, and an more convenient NumPy solution, which is especially useful if we attempt to transform a whole matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lq-5HSSFA5by"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WS7Y2PoPA5by"
   },
   "source": [
    "Just to recall the equations that we are using:\n",
    "\n",
    "Standardization: \\begin{equation} z = \\frac{x - \\mu}{\\sigma} \\end{equation} \n",
    "\n",
    "with mean:  \n",
    "\n",
    "\\begin{equation}\\mu = \\frac{1}{N} \\sum_{i=1}^N (x_i)\\end{equation}\n",
    "\n",
    "and standard deviation:  \n",
    "\n",
    "\\begin{equation}\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2}\\end{equation}\n",
    "\n",
    "\n",
    "Min-Max scaling: \\begin{equation} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} \\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lógica del bucle for\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8np6BxSWA5bz"
   },
   "source": [
    "### Pure Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "5XpINxXEA5bz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Standardization\n",
    "\n",
    "\n",
    "\n",
    "# Min-Max scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2O6zScLA5b1"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bbwi029aA5b1"
   },
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "jtpk8DKAA5b2",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Standardization\n",
    "\n",
    "\n",
    "\n",
    "# Min-Max scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NS5sTXR9A5b4"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDx1mgDqA5b4"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5-Nh9EKA5b4"
   },
   "source": [
    "Just to make sure that our code works correctly, let us plot the results via matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "b4j8TR7uA5b4",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "6IruhpK7A5b6",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "d443f3f0-bea5-46ab-b44d-35debc0a5624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nfrom matplotlib import pyplot as plt\\n\\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,5)) #devuelve fig, ax con 4 subplots\\n\\ny_pos = [0 for i in range(len(x))]\\n\\nax1.scatter(z_scores, y_pos, color='g')\\nax1.set_title('Python standardization', color='g')\\n\\nax2.scatter(minmax, y_pos, color='g')\\nax2.set_title('Python Min-Max scaling', color='g')\\n\\nax3.scatter(z_scores_np, y_pos, color='b')\\nax3.set_title('Python NumPy standardization', color='b')\\n\\nax4.scatter(np_minmax, y_pos, color='b')\\nax4.set_title('Python NumPy Min-Max scaling', color='b')\\n    \\nplt.tight_layout() # para ajustar la visualización\\n\\nfor ax in (ax1, ax2, ax3, ax4):\\n    ax.get_yaxis().set_visible(False)\\n    ax.grid()\\n\\nplt.show()\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,5)) #devuelve fig, ax con 4 subplots\n",
    "\n",
    "y_pos = [0 for i in range(len(x))]\n",
    "\n",
    "ax1.scatter(z_scores, y_pos, color='g')\n",
    "ax1.set_title('Python standardization', color='g')\n",
    "\n",
    "ax2.scatter(minmax, y_pos, color='g')\n",
    "ax2.set_title('Python Min-Max scaling', color='g')\n",
    "\n",
    "ax3.scatter(z_scores_np, y_pos, color='b')\n",
    "ax3.set_title('Python NumPy standardization', color='b')\n",
    "\n",
    "ax4.scatter(np_minmax, y_pos, color='b')\n",
    "ax4.set_title('Python NumPy Min-Max scaling', color='b')\n",
    "    \n",
    "plt.tight_layout() # para ajustar la visualización\n",
    "\n",
    "for ax in (ax1, ax2, ax3, ax4):\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos lo mismo, más fácil usando librerías\n",
    "# estandarizado con valores con media cero y desviación típica 1\n",
    "# escalado con valores entre 0 y 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTNz1_IkA5b8"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6um4V_iA5b8"
   },
   "source": [
    "## The effect of standardization on PCA in a pattern classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTq1SVSAA5b8"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-F-HCF4BA5b8"
   },
   "source": [
    "Earlier, I mentioned the Principal Component Analysis (PCA) as an example where standardization is crucial, since it is \"analyzing\" the variances of the different features.  \n",
    "Now, let us see how the standardization affects PCA and a following supervised classification on the whole wine dataset.\n",
    "\n",
    "\n",
    "In the following section, we will go through the following steps:\n",
    "\n",
    "- Reading in the dataset  \n",
    "- Dividing the dataset into a separate training and test dataset  \n",
    "- Standardization of the features    \n",
    "- Principal Component Analysis (PCA) to reduce the dimensionality   \n",
    "- Training a naive Bayes classifier  \n",
    "- Evaluating the classification accuracy with and without standardization  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oSY24Z_A5b9"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3M1X8HHA5b9"
   },
   "source": [
    "### Reading in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5T-7YUUA5b9"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "zIt2mvEuA5b-",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nimport pandas as pd\\n\\ndf = pd.io.parsers.read_csv(\\n    'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', \\n    header=None,\\n    )\\n\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.io.parsers.read_csv(\n",
    "    'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', \n",
    "    header=None,\n",
    "    )\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kh6qRNm2A5b_"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbVv7yTwA5b_"
   },
   "source": [
    "### Dividing the dataset into a separate training and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpXqmudhA5cA"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZC9-h4dQA5cA"
   },
   "source": [
    "In this step, we will randomly divide the wine dataset into a training dataset and a test dataset where the training dataset will contain 70% of the samples and the test dataset will contain 30%, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "vsF3FuqaA5cA",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "################# COMPLETAR\n",
    "################# COMPLETAR\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine,\n",
    "#    test_size=0.30, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r323qCDcA5cC"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0pb9XGmA5cD"
   },
   "source": [
    "### Feature Scaling - Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84EHxfyEA5cD"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "tGz9RszbA5cE",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JE2ea7cNA5cF"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktmHpb4jA5cF"
   },
   "source": [
    "### Dimensionality reduction via Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GbAS1qTIA5cG"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5FrcxB8SA5cG"
   },
   "source": [
    "Now, we perform a PCA on the standardized and the non-standardized datasets to transform the dataset onto a 2-dimensional feature subspace.  \n",
    "In a real application, a procedure like cross-validation would be done in order to find out what choice of features would yield a optimal balance between \"preserving information\" and \"overfitting\" for different classifiers. However, we will omit this step since we don't want to train a perfect classifier here, but merely compare the effects of standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "ZWwGJezXA5cG",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# on non-standardized data\n",
    "\n",
    "\n",
    "\n",
    "# om standardized data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-plJ_efA5cI"
   },
   "source": [
    "Let us quickly visualize how our new feature subspace looks like (note that class labels are not considered in a PCA - in contrast to a Linear Discriminant Analysis - but I will add them in the plot for clarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "nzRnH2nnA5cI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "EtjBg5VXA5cJ",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "90758540-3c9b-4bc9-daea-bfd7425d98a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nfrom matplotlib import pyplot as plt\\n\\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,4))\\n\\n\\nfor l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')):\\n    ax1.scatter(X_train[y_train==l, 0], X_train[y_train==l, 1],\\n        color=c, \\n        label='class %s' %l, \\n        alpha=0.5,\\n        marker=m\\n        )\\n\\nfor l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')):\\n    ax2.scatter(X_train_std[y_train==l, 0], X_train_std[y_train==l, 1],\\n        color=c, \\n        label='class %s' %l, \\n        alpha=0.5,\\n        marker=m\\n        )\\n\\nax1.set_title('Transformed NON-standardized training dataset after PCA')    \\nax2.set_title('Transformed standardized training dataset after PCA')    \\n    \\nfor ax in (ax1, ax2):\\n\\n    ax.set_xlabel('1st principal component')\\n    ax.set_ylabel('2nd principal component')\\n    ax.legend(loc='upper right')\\n    ax.grid()\\n#################### COMPLETAR\\n\\nplt.show()\\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "\n",
    "for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax1.scatter(X_train[y_train==l, 0], X_train[y_train==l, 1],\n",
    "        color=c, \n",
    "        label='class %s' %l, \n",
    "        alpha=0.5,\n",
    "        marker=m\n",
    "        )\n",
    "\n",
    "for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax2.scatter(X_train_std[y_train==l, 0], X_train_std[y_train==l, 1],\n",
    "        color=c, \n",
    "        label='class %s' %l, \n",
    "        alpha=0.5,\n",
    "        marker=m\n",
    "        )\n",
    "\n",
    "ax1.set_title('Transformed NON-standardized training dataset after PCA')    \n",
    "ax2.set_title('Transformed standardized training dataset after PCA')    \n",
    "    \n",
    "for ax in (ax1, ax2):\n",
    "\n",
    "    ax.set_xlabel('1st principal component')\n",
    "    ax.set_ylabel('2nd principal component')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid()\n",
    "#################### COMPLETAR\n",
    "\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LwQ76MV7A5cL"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaQL2nhKA5cL"
   },
   "source": [
    "### Training a naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UcKl4z9A5cL"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EvpEiahA5cM"
   },
   "source": [
    "We will use a naive Bayes classifier for the classification task. If you are not familiar with it, the term \"naive\" comes from the assumption that all features are \"independent\".  \n",
    "All in all, it is a simple but robust classifier based on Bayes' rule\n",
    "\n",
    "Bayes' Rule:\n",
    "\n",
    "\n",
    "\\begin{equation} P(\\omega_j|x) = \\frac{p(x|\\omega_j) * P(\\omega_j)}{p(x)} \\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "- &omega;:  class label  \n",
    "- *P(&omega;|x)*: the posterior probability\n",
    "- *p(x|&omega;)*: prior probability (or likelihood)\n",
    "\n",
    "and the **decsion rule:**\n",
    "\n",
    "Decide $ \\omega_1 $ if $ P(\\omega_1|x) > P(\\omega_2|x) $ else decide $ \\omega_2 $.\n",
    "<br>\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\Rightarrow \\frac{p(x|\\omega_1) * P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) * P(\\omega_2)}{p(x)}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "I don't want to get into more detail about Bayes' rule in this article, but if you are interested in a more detailed collection of examples, please have a look at the [Statistical Patter Classification](https://github.com/rasbt/pattern_classification#statistical-pattern-recognition-examples) in my pattern classification repository.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "hSqQVmqeA5cM",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# on non-standardized data\n",
    "\n",
    "\n",
    "# on standardized data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFg3YvJiA5cO"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sw0bfCQOA5cO"
   },
   "source": [
    "### Evaluating the classification accuracy with and without standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_npBWgGA5cP"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "HMYXSLpMA5cP",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "3c570ec4-ce79-46d3-bdc9-021d84440d56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn import metrics\\n\\npred_train = gnb.predict(X_train)\\n\\nprint('\\nPrediction accuracy for the training dataset')\\nprint('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train)))\\n\\npred_test = gnb.predict(X_test)\\n\\nprint('\\nPrediction accuracy for the test dataset')\\nprint('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn import metrics\n",
    "\n",
    "pred_train = gnb.predict(X_train)\n",
    "\n",
    "print('\\nPrediction accuracy for the training dataset')\n",
    "print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train)))\n",
    "\n",
    "pred_test = gnb.predict(X_test)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "xMMVrGHwA5cQ",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "70bb1409-4382-41bd-a5b4-1bd5c39cce1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npred_train_std = gnb_std.predict(X_train_std)\\n\\nprint('\\nPrediction accuracy for the training dataset')\\nprint('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\\n\\npred_test_std = gnb_std.predict(X_test_std)\\n\\nprint('\\nPrediction accuracy for the test dataset')\\nprint('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pred_train_std = gnb_std.predict(X_train_std)\n",
    "\n",
    "print('\\nPrediction accuracy for the training dataset')\n",
    "print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n",
    "\n",
    "pred_test_std = gnb_std.predict(X_test_std)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p5dCoVVBA5cS"
   },
   "source": [
    "As we can see, the standardization prior to the PCA definitely led to an decrease in the empirical error rate on classifying samples from test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YIhA2XFAA5cS"
   },
   "source": [
    "\n",
    "# Feature transformations\n",
    "\n",
    "### Normalization and changing distribution\n",
    "\n",
    "Monotonic feature transformation is critical for some algorithms and has no effect on others. This is one of the reasons for the increased popularity of decision trees and all its derivative algorithms (random forest, gradient boosting). Not everyone can or want to tinker with transformations, and these algorithms are robust to unusual distributions.\n",
    "\n",
    "There are also purely engineering reasons: `np.log` is a way of dealing with large numbers that do not fit in `np.float64`. This is an exception rather than a rule; often it's driven by the desire to adapt the dataset to the requirements of the algorithm. Parametric methods usually require a minimum of symmetric and unimodal distribution of data, which is not always given in real data. There may be more stringent requirements; recall [our earlier article about linear models](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220).\n",
    "\n",
    "However, data requirements are imposed not only by parametric methods; [K nearest neighbors](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd) will predict complete nonsense if features are not normalized e.g. when one distribution is located in the vicinity of zero and does not go beyond (-1, 1) while the other’s range is on the order of hundreds of thousands.\n",
    "\n",
    "A simple example: suppose that the task is to predict the cost of an apartment from two variables — the distance from city center and the number of rooms. The number of rooms rarely exceeds 5 whereas the distance from city center can easily be in the thousands of meters.\n",
    "\n",
    "The simplest transformation is Standard Scaling (or Z-score normalization):\n",
    "\n",
    "$$ \\large z= \\frac{x-\\mu}{\\sigma} $$\n",
    "\n",
    "Note that Standard Scaling does not make the distribution normal in the strict sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.382748Z",
     "start_time": "2018-03-15T14:06:29.338320Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ii0vJbW9A5cS",
    "outputId": "2473b257-6cac-4d2a-b4c4-a29e73ef984a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.509590Z",
     "start_time": "2018-03-15T14:06:29.385020Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1GIoJ9EMA5cU",
    "outputId": "8cba4cd8-a5e1-4afa-f44d-204093db0c04"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hipótesis nula: $\\sf{H_{0}}$\n",
    "Es la hipótesis por defecto. En este caso: los datos vienen de una Gaussiana.\n",
    "El p-valor es un estadístico que nos dice si es probable que la hipótesis nula sea cierta o sea falsa.\n",
    "Si p-valor <= 0.05 entonces rechazo $\\sf{H_{0}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTXNfTkVA5cW"
   },
   "source": [
    "But, to some extent, it protects against outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.602528Z",
     "start_time": "2018-03-15T14:06:29.511150Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "mXGrFSuSA5cW",
    "outputId": "1f6e8858-22b5-4849-f7a1-eff91b93b44a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.713603Z",
     "start_time": "2018-03-15T14:06:29.605288Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ucv29P_NA5cY",
    "outputId": "c6990fe9-2c3a-4386-ee0f-f4078c1278a3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAVi_YpfA5cZ"
   },
   "source": [
    "Another fairly popular option is MinMax Scaling, which brings all the points within a predetermined interval (typically (0, 1)).\n",
    "\n",
    "$$ \\large X_{norm}=\\frac{X-X_{min}}{X_{max}-X_{min}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.855619Z",
     "start_time": "2018-03-15T14:06:29.716000Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Jb-cyaDiA5ca",
    "outputId": "3ff69e57-e9f4-4395-dc30-d2e3878bf2e7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.955155Z",
     "start_time": "2018-03-15T14:06:29.857042Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ux8Gz4qkA5cb",
    "outputId": "b05664b0-d9a1-4bf4-8522-3b0b663adb3f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVVWyc1XA5cd"
   },
   "source": [
    "StandardScaling and MinMax Scaling have similar applications and are often more or less interchangeable. However, if the algorithm involves the calculation of distances between points or vectors, the default choice is StandardScaling. But MinMax Scaling is useful for visualization by bringing features within the interval (0, 255).\n",
    "\n",
    "If we assume that some data is not normally distributed but is described by the [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution), it can easily be transformed to a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:30.067680Z",
     "start_time": "2018-03-15T14:06:29.957011Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GAtRlFUtA5cd",
    "outputId": "a690a04e-58f5-4dad-b137-b78327c23a8d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:30.180348Z",
     "start_time": "2018-03-15T14:06:30.069180Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bX9QqUi-A5cg",
    "outputId": "2c9e38a5-0353-4e9c-e640-f3d1119d7038"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qD8MpPp4A5ch"
   },
   "source": [
    "The lognormal distribution is suitable for describing salaries, price of securities, urban population, number of comments on articles on the internet, etc. However, to apply this procedure, the underlying distribution does not necessarily have to be lognormal; you can try to apply this transformation to any distribution with a heavy right tail. Furthermore, one can try to use other similar transformations, formulating their own hypotheses on how to approximate the available distribution to a normal. Examples of such transformations are [Box-Cox transformation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html) (logarithm is a special case of the Box-Cox transformation) or [Yeo-Johnson transformation](https://gist.github.com/mesgarpour/f24769cd186e2db853957b10ff6b7a95) (extends the range of applicability to negative numbers). In addition, you can also try adding a constant to the feature — `np.log (x + const)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWjJU2MuA5ci"
   },
   "source": [
    "# Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MylAxHQjA5ci",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "134cc5c4-9962-4879-ae97-9114ad3069b4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSTrEM6AA5cj"
   },
   "source": [
    "## Fixed-width binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gB85yDDkA5ck"
   },
   "source": [
    "### Developer age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "1GDAI6NIA5ck",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "5e603cf7-fea4-4078-f046-f4ea26e6976e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Bi6STxGA5cl"
   },
   "source": [
    "### Binning based on rounding\n",
    "\n",
    "``` \n",
    "Age Range: Bin\n",
    "---------------\n",
    " 0 -  9  : 0\n",
    "10 - 19  : 1\n",
    "20 - 29  : 2\n",
    "30 - 39  : 3\n",
    "40 - 49  : 4\n",
    "50 - 59  : 5\n",
    "60 - 69  : 6\n",
    "  ... and so on\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "yZEV-G1pA5cm",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f9fca4c7-ce4a-4041-eeae-4c3d8b4c2782"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PL6KvuyA5cn"
   },
   "source": [
    "### Binning based on custom ranges\n",
    "\n",
    "``` \n",
    "Age Range : Bin\n",
    "---------------\n",
    " 0 -  15  : 1\n",
    "16 -  30  : 2\n",
    "31 -  45  : 3\n",
    "46 -  60  : 4\n",
    "61 -  75  : 5\n",
    "75 - 100  : 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "7cfltQpAA5cn",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b08a956d-a679-43c4-c278-37aa88806c88"
   },
   "outputs": [],
   "source": [
    "# cut crea bins\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BshqgrYVA5cp"
   },
   "source": [
    "## Quantile based binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "sz1qddTZA5cp",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ae6721cf-edfb-4b17-d7b5-ebb4a236f494"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "2xbYwRQ1A5cq",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "7c1db47b-6e49-417f-aab1-cea497a95c6f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "bKxprAEQA5cs",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "2b4e492b-b2d3-4de1-84c4-cf2cd5e840fd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "bNJrNhoKA5ct",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "9c56378e-e5c3-466d-a9e5-87a7350a8deb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "MlsDrooFA5cv",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "95de9631-3523-40f4-aef2-e5e8b5bbd460"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8K_bN1LEA5cx"
   },
   "source": [
    "# Mathematical Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NE2FL4jvA5cy"
   },
   "source": [
    "## Log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "cmw3Xy7RA5cy",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "d936e56e-e785-43f2-d3c9-fb50a75d5686"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "ueviu9sNA5c0",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "dc45ee44-c02f-4687-e9e3-d9cba1f4d3c9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdWPJCx4A5c2"
   },
   "source": [
    "## Box–Cox transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "KFS3M3P2A5c2",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f982aee2-1846-48a4-bca8-4ba54e5bd4d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda value: 0.11799122497648248\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# get optimal lambda value from non null income values\n",
    "income = np.array(fcc_survey_df['Income'])\n",
    "income_clean = income[~np.isnan(income)] # los no nan\n",
    "l, opt_lambda = stats.boxcox(income_clean)\n",
    "print('Optimal lambda value:', opt_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "w35rcollA5c3",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "78db1834-14fc-4a7a-a6f7-22ee0e8bfeaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alberto.Romero\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:1038: RuntimeWarning: invalid value encountered in less_equal\n",
      "  if any(x <= 0):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID.x</th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Income_log</th>\n",
       "      <th>Income_boxcox_lambda_0</th>\n",
       "      <th>Income_boxcox_lambda_opt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9368291c93d5d5f5c8cdb1a575e18bec</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>8.699681</td>\n",
       "      <td>8.699681</td>\n",
       "      <td>15.180667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dd0e77eab9270e4b67c19b0d6bbf621b</td>\n",
       "      <td>34.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>10.596660</td>\n",
       "      <td>10.596660</td>\n",
       "      <td>21.115340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7599c0aa0419b59fd11ffede98a3665d</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>10.373522</td>\n",
       "      <td>10.373522</td>\n",
       "      <td>20.346418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6dff182db452487f07a47596f314bddc</td>\n",
       "      <td>35.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>10.596660</td>\n",
       "      <td>10.596660</td>\n",
       "      <td>21.115340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9dc233f8ed1c6eb2432672ab4bb39249</td>\n",
       "      <td>33.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>11.289794</td>\n",
       "      <td>11.289794</td>\n",
       "      <td>23.637128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ID.x   Age   Income  Income_log  \\\n",
       "4  9368291c93d5d5f5c8cdb1a575e18bec  20.0   6000.0    8.699681   \n",
       "5  dd0e77eab9270e4b67c19b0d6bbf621b  34.0  40000.0   10.596660   \n",
       "6  7599c0aa0419b59fd11ffede98a3665d  23.0  32000.0   10.373522   \n",
       "7  6dff182db452487f07a47596f314bddc  35.0  40000.0   10.596660   \n",
       "8  9dc233f8ed1c6eb2432672ab4bb39249  33.0  80000.0   11.289794   \n",
       "\n",
       "   Income_boxcox_lambda_0  Income_boxcox_lambda_opt  \n",
       "4                8.699681                 15.180667  \n",
       "5               10.596660                 21.115340  \n",
       "6               10.373522                 20.346418  \n",
       "7               10.596660                 21.115340  \n",
       "8               11.289794                 23.637128  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcc_survey_df['Income_boxcox_lambda_0'] = stats.boxcox((1+fcc_survey_df['Income']), \n",
    "                                                         lmbda=0)\n",
    "fcc_survey_df['Income_boxcox_lambda_opt'] = stats.boxcox(fcc_survey_df['Income'], \n",
    "                                                           lmbda=opt_lambda)\n",
    "fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_log', \n",
    "               'Income_boxcox_lambda_0', 'Income_boxcox_lambda_opt']].iloc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "xEjacahKA5c5",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "a41adc5b-3a97-49c0-e59a-dd3a11e0777b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(24, 450, '$\\\\mu$=20.65')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "income_boxcox_mean = np.round(np.mean(fcc_survey_df['Income_boxcox_lambda_opt']), 2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fcc_survey_df['Income_boxcox_lambda_opt'].hist(bins=30, color='#A9C5D3')\n",
    "plt.axvline(income_boxcox_mean, color='r')\n",
    "ax.set_title('Developer Income Histogram after Box–Cox Transform', fontsize=12)\n",
    "ax.set_xlabel('Developer Income (Box–Cox transform)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.text(24, 450, r'$\\mu$='+str(income_boxcox_mean), fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oe7FUBCA5c8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "1-Numerical_Features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
