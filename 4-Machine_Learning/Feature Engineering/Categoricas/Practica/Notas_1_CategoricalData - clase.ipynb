{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0x9BSWIok-4C"
   },
   "source": [
    "# Categorical Values\n",
    "\n",
    "A categorical variable, as the name suggests, is used to represent categories or labels. For instance, a categorical variable could represent major cities in the world, the four seasons in a year, or the industry (oil, travel, technology) of a company. The number of category values is always finite in a real-world dataset. The values may be represented numerically. However, unlike other numeric variables, the values of a categorical variable cannot be ordered with respect to one another. (Oil is neither greater than nor less than travel as an industry type.) They are called nonordinal.\n",
    "\n",
    "A simple question can serve as litmus test for whether something should be a categorical variable: “Does it matter how different two values are, or only that they are different?” A stock price of 500 € is five times higher than a price of 100 €. So, stock price should be represented by a continuous numeric variable. The industry of the company (oil, travel, tech, etc.), on the other hand, should probably be categorical.\n",
    "\n",
    "Large categorical variables are particularly common in transactional records. For instance, many web services track users using an ID, which is a categorical variable with hundreds to hundreds of millions of values, depending on the number of unique users of the service. \n",
    "\n",
    "The IP address of an internet transaction is another example of a large categorical variable. They are categorical variables because, even though user IDs and IP addresses are numeric, their magnitude is usually not relevant to the task at hand. For instance, the IP address might be relevant when doing fraud detection on individual transactions—some IP addresses or subnets may generate more fraudulent transactions than others. But a subnet of 164.203.x.x is not inherently more fraudulent than 164.202.x.x; the numeric value of the subnet does not matter.\n",
    "\n",
    "The vocabulary of a document corpus can be interpreted as a large categorical variable, with the categories being unique words. It can be computationally expensive to represent so many distinct categories. If a category (e.g., word) appears multiple times in a data point (document), then we can represent it as a count, and represent all of the categories through their count statistics. \n",
    "\n",
    "This is called bin counting. We start this discussion with common representations of categorical variables, and eventually meander our way to a discussion of bin counting for large categorical variables, which are very common in modern datasets.\n",
    "\n",
    "# Encoding Categorical Variables\n",
    "\n",
    "The categories of a categorical variable are usually not numeric.1 For example, eye color can be “black,” “blue,” “brown,” etc. Thus, an encoding method is needed to turn these nonnumeric categories into numbers. It is tempting to simply assign an integer, say from 1 to k, to each of k possible categories—but the resulting values would be orderable against each other, which should not be permissible for categories. So, let’s look at some alternatives.\n",
    "One-Hot Encoding\n",
    "\n",
    "A better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be “on.” This is called one-hot encoding, and it is implemented in scikit-learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k. Table 5-1 shows an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeUyHEnAk-4D"
   },
   "source": [
    "![texto alternativo](https://drive.google.com/uc?id=1qL2klfmXws6LXfhgLD2WPLT544cr9BlT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcxiSGpNk-4E"
   },
   "source": [
    "One-hot encoding is very simple to understand, but it uses one more bit than is strictly necessary. If we see that k–1 of the bits are 0, then the last bit must be 1 because the variable must take on one of the k values. Mathematically, one can write this constraint as “the sum of all bits must be equal to 1”:\n",
    "e 1 + e 2 + . . . + e k = 1\n",
    "\n",
    "Thus, we have a linear dependency on our hands. Linear dependent features, as we discovered in Chapter 4, are slightly annoying because they mean that the trained linear models will not be unique. Different linear combinations of the features can make the same predictions, so we would need to jump through extra hoops to understand the effect of a feature on the prediction.\n",
    "\n",
    "\n",
    "## Dummy Coding\n",
    "\n",
    "The problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k–1. Dummy coding2 removes the extra degree of freedom by using only k–1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.\n",
    "Table 5-2. Dummy coding of a category of three cities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t72LvZ5-k-4E"
   },
   "source": [
    "The outcome of modeling with dummy coding is more interpretable than with one-hot encoding. This is easy to see in a simple linear regression problem. Suppose we have some data about apartment rental prices in three cities: San Francisco, New York, and Seattle (see Table 5-3).\n",
    "Table 5-3. Toy dataset of apartment prices in three cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJwqnblZk-4F"
   },
   "outputs": [],
   "source": [
    "# 5 min\n",
    "# crear DataFrame con estas ciudades\n",
    "# SF, SF, SF, NYC, NYC, NYC, Seattle, Seattle, Seattle\n",
    "# y con estas rentas\n",
    "# 3999, 4000, 4001, 3499, 3500, 3501, 2499, 2500, 2501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cH6Wx51fk-4I",
    "outputId": "2051375c-1bbe-4bf1-d0fe-701ff687a2ca"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlRfNyUYk-4L",
    "outputId": "d8197e1a-1027-44d4-8a6c-ad8a668f756b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Rent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SF</td>\n",
       "      <td>3999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SF</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SF</td>\n",
       "      <td>4001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NYC</td>\n",
       "      <td>3499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NYC</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NYC</td>\n",
       "      <td>3501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>2499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>2501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      City  Rent\n",
       "0       SF  3999\n",
       "1       SF  4000\n",
       "2       SF  4001\n",
       "3      NYC  3499\n",
       "4      NYC  3500\n",
       "5      NYC  3501\n",
       "6  Seattle  2499\n",
       "7  Seattle  2500\n",
       "8  Seattle  2501"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'City': ['SF', 'SF', 'SF', 'NYC', 'NYC', 'NYC', 'Seattle', 'Seattle', 'Seattle'],\n",
    "                   'Rent': [3999, 4000, 4001, 3499, 3500, 3501, 2499, 2500, 2501]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a92VojSVk-4O"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rent</th>\n",
       "      <th>City_NYC</th>\n",
       "      <th>City_SF</th>\n",
       "      <th>City_Seattle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3999</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3499</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3501</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2499</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2501</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rent  City_NYC  City_SF  City_Seattle\n",
       "0  3999         0        1             0\n",
       "1  4000         0        1             0\n",
       "2  4001         0        1             0\n",
       "3  3499         1        0             0\n",
       "4  3500         1        0             0\n",
       "5  3501         1        0             0\n",
       "6  2499         0        0             1\n",
       "7  2500         0        0             1\n",
       "8  2501         0        0             1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_df = pd.get_dummies(df, prefix = ['City'])\n",
    "one_hot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a92VojSVk-4O"
   },
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iA6n0Sajk-4Q",
    "outputId": "a606e274-f3d6-473d-90fc-34c7dd1c9af5"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['city_NYC', 'city_SF', 'city_Seattle'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ws/4s9tjczx71l06gt4dkp8q4vw0000gn/T/ipykernel_10272/291942658.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlin_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'city_NYC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'city_SF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'city_Seattle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/TheBridgeBootcamp/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/TheBridgeBootcamp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[0;32m/opt/anaconda3/envs/TheBridgeBootcamp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['city_NYC', 'city_SF', 'city_Seattle'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "lin_reg.fit(one_hot_df[['city_NYC', 'city_SF', 'city_Seattle']], one_hot_df['Rent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_IxVcEEk-4S",
    "outputId": "faab7bfe-4c6e-4178-a748-d0512bb3f2b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 166.66666667,  666.66666667, -833.33333333])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5 min\n",
    "# buscar los coeficientes de la regresión lineal\n",
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CT9vXytgk-4V",
    "outputId": "bdf8ae9b-7d4f-4b36-820a-552f232fff4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3333.3333333333335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1 min el intercept\n",
    "lin_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kk4a6Erkk-4X"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding weights + intercept\n",
    "w1 = lin_reg.coef_\n",
    "b1 = lin_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RllE2o72k-4Z",
    "outputId": "98f3aa19-cc17-4c0e-df3f-90cef929d29a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3333.3333333333335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calcular la media de la renta\n",
    "# 1 min\n",
    "df['Rent'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUBSw1Myk-4b",
    "outputId": "97055154-1091-4a5b-f66d-66b785b36379"
   },
   "outputs": [],
   "source": [
    "# hacer un dummy encoder (quitando una columna)\n",
    "# 5 min\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iA6n0Sajk-4Q",
    "outputId": "a606e274-f3d6-473d-90fc-34c7dd1c9af5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_IxVcEEk-4S",
    "outputId": "faab7bfe-4c6e-4178-a748-d0512bb3f2b0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CT9vXytgk-4V",
    "outputId": "bdf8ae9b-7d4f-4b36-820a-552f232fff4e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kk4a6Erkk-4X"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding weights + intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RllE2o72k-4Z",
    "outputId": "98f3aa19-cc17-4c0e-df3f-90cef929d29a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUBSw1Myk-4b",
    "outputId": "97055154-1091-4a5b-f66d-66b785b36379"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4-abR0ak-4d",
    "outputId": "bc442e82-8bce-4a56-ec12-a127235e0f43"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMmIO989k-4g",
    "outputId": "88abca92-1b84-418c-e680-f22dcec61cb2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1eJC3XUk-4i",
    "outputId": "a8d0f8c2-4ad0-47c9-c845-8180bfa88140"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TmH_GCbk-4l"
   },
   "outputs": [],
   "source": [
    "# Dummy coding weights + intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zdNV5hok-4n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wRvlap4k-4p",
    "outputId": "2a566d2b-f4ca-493a-d30c-297383d09c74"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sul0ot6tk-4s",
    "outputId": "b2537e81-f0f7-4ba8-84c8-b2e6d7d1c7b9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOXcBgiek-4u",
    "outputId": "78c66971-6c07-49ce-feb1-780e897ced3a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzhHG57hk-4w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# illustration of rental price in cities\\n%matplotlib inline\\nimport numpy as np\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport pandas.util.testing as tm\\nsns.set(style=\"whitegrid\", font_scale=1.4, color_codes=True)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# illustration of rental price in cities\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas.util.testing as tm\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4, color_codes=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDninYsUk-4x",
    "outputId": "7b149c52-87b1-4846-b479-d02e5ba535d9"
   },
   "outputs": [],
   "source": [
    "# sns.swarmplot(x=\"City\", y=\"Rent\", data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSJ7eE5sk-4z",
    "outputId": "557f3477-67dd-48d8-f4d6-ba543d5d847c"
   },
   "outputs": [],
   "source": [
    "# print('One-hot encoding weights: ' ,w1, ' and intercept: ', b1)\n",
    "# print('Dummy encoding weights: ' ,w2, ' and intercept: ', b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBNTUYMfk-41",
    "outputId": "c5f9da8f-7224-44af-f55f-fcfc9d9d2bbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# geometry of one-hot vs. dummy encoding\\n\\n# Create a list of values in the best fit line for one-hot encoding\\none_hot_y = [((w1[0] * one_hot_df.city_NYC[i]) + \\n              (w1[1] * one_hot_df.city_SF[i]) +\\n              (w1[2] * one_hot_df.city_Seattle[i]) + b1) \\n             for i in range(0,one_hot_df.shape[0])]\\n\\n# Create a list of values in the best fit line for dummy coding\\ndummy_y = [((w2[0] * dummy_df.city_SF[i]) +\\n            (w2[1] * dummy_df.city_Seattle[i]) + b2)\\n           for i in range(0,dummy_df.shape[0])]\\n\\nprint(one_hot_y)\\nprint(dummy_y)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# geometry of one-hot vs. dummy encoding\n",
    "\n",
    "# Create a list of values in the best fit line for one-hot encoding\n",
    "one_hot_y = [((w1[0] * one_hot_df.city_NYC[i]) + \n",
    "              (w1[1] * one_hot_df.city_SF[i]) +\n",
    "              (w1[2] * one_hot_df.city_Seattle[i]) + b1) \n",
    "             for i in range(0,one_hot_df.shape[0])]\n",
    "\n",
    "# Create a list of values in the best fit line for dummy coding\n",
    "dummy_y = [((w2[0] * dummy_df.city_SF[i]) +\n",
    "            (w2[1] * dummy_df.city_Seattle[i]) + b2)\n",
    "           for i in range(0,dummy_df.shape[0])]\n",
    "\n",
    "print(one_hot_y)\n",
    "print(dummy_y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iRdJWOvYk-43"
   },
   "source": [
    "\n",
    "Pros and Cons of Categorical Variable Encodings\n",
    "\n",
    "One-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiple valid models for the same problem. The nonuniqueness is sometimes problematic for interpretation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the all-zeros vector, and the output should be the overall mean of the target variable.\n",
    "\n",
    "Dummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange.\n",
    "\n",
    "Effect coding avoids this problem by using a different code for the reference category, but the vector of all –1’s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding.\n",
    "\n",
    "All three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables. \n",
    "\n",
    "## Dealing with Large Categorical Variables\n",
    "\n",
    "Automated data collection on the internet can generate large categorical variables. This is common in applications such as targeted advertising and fraud detection.\n",
    "\n",
    "In targeted advertising, the task is to match a user with a set of ads. Features include the user ID, the website domain for the ad, the search query, the current page, and all possible pairwise conjunctions of those features. (The query is a text string that can be chopped up and turned into the usual text features. However, queries are generally short and are often composed of phrases, so the best course of action in this case is usually to keep them intact, or pass them through a hash function to make storage and comparisons easier. We will discuss hashing in more detail later.) Each of these is a very large categorical variable. The challenge is to find a good feature representation that is memory efficient, yet produces accurate models that are fast to train.\n",
    "\n",
    "Existing solutions can be categorized (haha) thus:\n",
    "\n",
    "    Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.\n",
    "    Compress the features. There are two choices:\n",
    "        Feature hashing, popular with linear models\n",
    "        Bin counting, popular with linear models as well as trees\n",
    "\n",
    "Using the vanilla one-hot encoding is a valid option. For Microsoft’s search advertising engine, Graepel et al. (2010) report using such binary-valued features in a Bayesian probit regression model that can be trained online using simple updates. Meanwhile, other groups argue for the compression approach. Researchers from Yahoo! swear by feature hashing (Weinberger et al., 2009), though McMahan et al. (2013) experimented with feature hashing on Google’s advertising engine and did not find significant improvements. Yet other folks at Microsoft are taken with the idea of bin counting (Bilenko, 2015).\n",
    "\n",
    "As we shall see, all of these ideas have pros and cons. We will first describe the solutions themselves, then discuss their trade-offs.\n",
    "\n",
    "## Feature Hashing\n",
    "\n",
    "A hash function is a deterministic function that maps a potentially unbounded integer to a finite integer range [1, m]. Since the input domain is potentially larger than the output range, multiple numbers may get mapped to the same output. This is called a collision. A uniform hash function ensures that roughly the same number of numbers are mapped into each of the m bins.\n",
    "\n",
    "Visually, we can think of a hash function as a machine that intakes numbered balls (keys) and routes them to one of m bins. Balls with the same number will always get routed to the same bin (see Figure 5-1). This maintains the feature space while reducing the storage and processing time during machine learning training and evaluation cycles.\n",
    "\n",
    "Hash functions can be constructed for any object that can be represented numerically (which is true for any data that can be stored on a computer): numbers, strings, complex structures, etc.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1cTY_mxkTAS33w9ck-Pn_8v67LspVGnTe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gz4j5b0yk-43"
   },
   "source": [
    "Feature hashing can be used for models that involve the inner product of feature vectors and coefficients, such as linear models and kernel methods. It has been demonstrated to be successful in the task of spam filtering (Weinberger et al., 2009). In the case of targeted advertising, McMahan et al. (2013) report not being able to get the prediction errors down to an acceptable level unless m is on the order of billions, which does not constitute enough saving in space.\n",
    "\n",
    "One downside to feature hashing is that the hashed features, being aggregates of original features, are no longer interpretable.\n",
    "\n",
    "In Example 5-5, we use the Yelp reviews dataset to demonstrate storage and interpretability trade-offs using scikit-learn’s FeatureHasher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cioioVKGk-44"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo de documento json\n",
    "# https://json.org/example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XjfqmzhFk-45",
    "outputId": "376951c0-94d6-46ea-e259-bd339ac358c9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Load the first 10000 reviews\n",
    "f = open('yelp_academic_dataset_review.json', encoding=\"utf8\")\n",
    "js = []\n",
    "for i in range(10000):\n",
    "    js.append(json.loads(f.readline()))\n",
    "f.close()\n",
    "review_df = pd.DataFrame(js)\n",
    "review_df.shape\n",
    "'''\n",
    "\n",
    "# leer 'review.csv'\n",
    "os.getcwd()\n",
    "review_df = pd.read_csv('./Ficheros/review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n",
       "      <td>OwjRMXRC0KyPrIlcjaXeFQ</td>\n",
       "      <td>-MhfebM0QIsKt87iDN-FNw</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "      <td>2015-04-15 05:21:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n",
       "      <td>nIJD_7ZXHq-FX8byPMOkMQ</td>\n",
       "      <td>lbrU8StCq3yDfr-QMnGrmQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "      <td>2013-12-07 03:16:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>LG2ZaYiOgpr2DK_90pYjNw</td>\n",
       "      <td>V34qejxNsCbcgD8C0HVk-Q</td>\n",
       "      <td>HQl28KMwrEKHqhFrrDqVNQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "      <td>2015-12-05 03:18:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i6g_oA9Yf9Y31qt0wibXpw</td>\n",
       "      <td>ofKDkJKXSKZXu5xJNGiiBQ</td>\n",
       "      <td>5JxlZaqCnk1MnbgRirs40Q</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "      <td>2011-05-27 05:30:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6TdNDKywdbjoTkizeMce8A</td>\n",
       "      <td>UgMW8bLE0QMJDCkQ1Ax5Mg</td>\n",
       "      <td>IS4cv902ykd8wj1TR0N3-A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "      <td>2017-01-14 21:56:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>PxLgGV56Hw4txlG6gKvapQ</td>\n",
       "      <td>1IyGLnESYghXsScyn3ltNA</td>\n",
       "      <td>z2JTN5PXemCRGtGbKiOvZw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I came here to get a pedicure. Worst experienc...</td>\n",
       "      <td>2015-10-07 23:45:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>3ykur79WxV27JxgIeaUyEw</td>\n",
       "      <td>IivADm5_qbGHYJvxPDDdLA</td>\n",
       "      <td>Jt28TYWanzKrJYYr0Tf1MQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Excellent service and excellent food! We ate b...</td>\n",
       "      <td>2017-06-17 18:25:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>q7vcvqY434k_Rw3X0eEk_g</td>\n",
       "      <td>5-U7VLg1OtAXLGY57npZLQ</td>\n",
       "      <td>90oH6tilpqsCkz7Dhcxejw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In line with and one of the best massages I ha...</td>\n",
       "      <td>2018-05-08 18:48:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>hd6KRCmVtFqwGluq3D3Nsw</td>\n",
       "      <td>9TNITWe9p-7qCdyAC0u5Nw</td>\n",
       "      <td>9IdnNV6Rq1ddFyWMdyAKrQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Virginia has been our vet for 6 years. She has...</td>\n",
       "      <td>2014-09-25 16:57:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>l6pSPefEqPaFSABqncNOCw</td>\n",
       "      <td>XChCfeJ6Yx2NDJIpIgRhyg</td>\n",
       "      <td>Ehq7wmTyxVdTJkv_MoRqmg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If the $5.99 lunch special proves to be as awe...</td>\n",
       "      <td>2012-05-29 01:19:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0               review_id                 user_id  \\\n",
       "0              0  xQY8N_XvtGbearJ5X4QryQ  OwjRMXRC0KyPrIlcjaXeFQ   \n",
       "1              1  UmFMZ8PyXZTY2QcwzsfQYA  nIJD_7ZXHq-FX8byPMOkMQ   \n",
       "2              2  LG2ZaYiOgpr2DK_90pYjNw  V34qejxNsCbcgD8C0HVk-Q   \n",
       "3              3  i6g_oA9Yf9Y31qt0wibXpw  ofKDkJKXSKZXu5xJNGiiBQ   \n",
       "4              4  6TdNDKywdbjoTkizeMce8A  UgMW8bLE0QMJDCkQ1Ax5Mg   \n",
       "...          ...                     ...                     ...   \n",
       "9995        9995  PxLgGV56Hw4txlG6gKvapQ  1IyGLnESYghXsScyn3ltNA   \n",
       "9996        9996  3ykur79WxV27JxgIeaUyEw  IivADm5_qbGHYJvxPDDdLA   \n",
       "9997        9997  q7vcvqY434k_Rw3X0eEk_g  5-U7VLg1OtAXLGY57npZLQ   \n",
       "9998        9998  hd6KRCmVtFqwGluq3D3Nsw  9TNITWe9p-7qCdyAC0u5Nw   \n",
       "9999        9999  l6pSPefEqPaFSABqncNOCw  XChCfeJ6Yx2NDJIpIgRhyg   \n",
       "\n",
       "                 business_id  stars  useful  funny  cool  \\\n",
       "0     -MhfebM0QIsKt87iDN-FNw    2.0       5      0     0   \n",
       "1     lbrU8StCq3yDfr-QMnGrmQ    1.0       1      1     0   \n",
       "2     HQl28KMwrEKHqhFrrDqVNQ    5.0       1      0     0   \n",
       "3     5JxlZaqCnk1MnbgRirs40Q    1.0       0      0     0   \n",
       "4     IS4cv902ykd8wj1TR0N3-A    4.0       0      0     0   \n",
       "...                      ...    ...     ...    ...   ...   \n",
       "9995  z2JTN5PXemCRGtGbKiOvZw    1.0       1      0     0   \n",
       "9996  Jt28TYWanzKrJYYr0Tf1MQ    5.0       0      0     0   \n",
       "9997  90oH6tilpqsCkz7Dhcxejw    5.0       0      0     0   \n",
       "9998  9IdnNV6Rq1ddFyWMdyAKrQ    5.0       1      0     1   \n",
       "9999  Ehq7wmTyxVdTJkv_MoRqmg    4.0       0      0     0   \n",
       "\n",
       "                                                   text                 date  \n",
       "0     As someone who has worked with many museums, I...  2015-04-15 05:21:16  \n",
       "1     I am actually horrified this place is still in...  2013-12-07 03:16:52  \n",
       "2     I love Deagan's. I do. I really do. The atmosp...  2015-12-05 03:18:11  \n",
       "3     Dismal, lukewarm, defrosted-tasting \"TexMex\" g...  2011-05-27 05:30:52  \n",
       "4     Oh happy day, finally have a Canes near my cas...  2017-01-14 21:56:57  \n",
       "...                                                 ...                  ...  \n",
       "9995  I came here to get a pedicure. Worst experienc...  2015-10-07 23:45:05  \n",
       "9996  Excellent service and excellent food! We ate b...  2017-06-17 18:25:56  \n",
       "9997  In line with and one of the best massages I ha...  2018-05-08 18:48:16  \n",
       "9998  Virginia has been our vet for 6 years. She has...  2014-09-25 16:57:16  \n",
       "9999  If the $5.99 lunch special proves to be as awe...  2012-05-29 01:19:21  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Yitj15Nk-48",
    "outputId": "80ccb0dc-2c4e-4fff-9ab9-ee32a6c6afec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4398\n"
     ]
    }
   ],
   "source": [
    "# we will define m as equal to the unique number of business_id\n",
    "m = len(review_df['business_id'].unique())\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYQ4GApHk-4-"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1zgBLs4k-4_"
   },
   "outputs": [],
   "source": [
    "# usad el constructor para crear un objeto FeatureHasher\n",
    "h = FeatureHasher(n_features=m, input_type='string')\n",
    "f = h.transform(review_df['business_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B26iwQt5k-5B",
    "outputId": "e0c2a8e0-8ca4-4001-e757-4eb41df149f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our pandas Series, in bytes:  790160\n",
      "Our hashed numpy array, in bytes:  64\n"
     ]
    }
   ],
   "source": [
    "# We can see how this will make a difference in the future by looking at the size of each\n",
    "from sys import getsizeof\n",
    "\n",
    "print('Our pandas Series, in bytes: ', getsizeof(review_df['business_id']))\n",
    "print('Our hashed numpy array, in bytes: ', getsizeof(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nPziZdlk-5D",
    "outputId": "be4448f2-521c-4cad-8e7e-486e191401ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-MhfebM0QIsKt87iDN-FNw',\n",
       " 'lbrU8StCq3yDfr-QMnGrmQ',\n",
       " 'HQl28KMwrEKHqhFrrDqVNQ',\n",
       " '5JxlZaqCnk1MnbgRirs40Q',\n",
       " 'IS4cv902ykd8wj1TR0N3-A']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['business_id'].unique().tolist()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kx7IXr4bk-5F",
    "outputId": "0c9626ca-66c5-4aeb-9fd3-7eb4890bc8d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-MhfebM0QIsKt87iDN-FNw', 'lbrU8StCq3yDfr-QMnGrmQ',\n",
       "       'HQl28KMwrEKHqhFrrDqVNQ', '5JxlZaqCnk1MnbgRirs40Q',\n",
       "       'IS4cv902ykd8wj1TR0N3-A'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['business_id'].unique()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.toarray()[f.toarray()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbW1gwz3k-5G"
   },
   "source": [
    "## Bin Counting\n",
    "\n",
    "Bin counting is one of the perennial rediscoveries in machine learning. It has been reinvented and used in a variety of applications, from ad click-through rate prediction to hardware branch prediction (Yeh and Patt, 1991; Lee et al., 1998; Chen et al., 2009; Li et al., 2010). Yet because it is a feature engineering technique and not a modeling or optimization method, there is no research paper on the topic. The most detailed description of the technique can be found in Misha Bilenko’s (2015) blog post “Big Learning Made Easy—with Counts!” and the associated slides.\n",
    "\n",
    "The idea of bin counting is deviously simple: rather than using the value of the categorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent. It is best illustrated with an example (see Table 5-6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMzAMxEck-5H"
   },
   "source": [
    "![texto alternativo](https://drive.google.com/uc?id=1Zz3RhCWdF-Zf_q0ro_Lrjqj_LZK8auJ_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUARA_V8k-5H"
   },
   "source": [
    "Bin counting assumes that historical data is available for computing the statistics. Table 5-6 contains aggregated historical counts for each possible value of the categorical variables. Based on the number of times the user “Alice” has clicked on any ad and the number of times she has not clicked, we can calculate the probability of her clicking on any ad. Similarly, we can compute the probability of a click for any query–ad domain combination. At training time, every time we see “Alice,” we can use her probability of click as the input feature to the model. The same goes for QueryHash–AdDomain pairs like “0x437a45e1, qux.net.”\n",
    "\n",
    "Suppose there were 10,000 users. One-hot encoding would generate a sparse vector of length 10,000, with a single 1 in the column that corresponds to the value of the current data point. Bin counting would encode all 10,000 binary columns as a single feature with a real value between 0 and 1.\n",
    "\n",
    "We can include other features in addition to the historical click-through probability: the raw counts themselves (number of clicks and nonclicks), the log-odds ratio, or any other derivatives of probability. Our example here is for predicting ad click-through rates, but the technique readily applies to general binary classification. It can also be readily extended to multiclass classification using the usual techniques to extend binary classifiers to multiclass; i.e., via one-against-many odds ratios or other multiclass label encodings.\n",
    "\n",
    "In short, bin counting converts a categorical variable into statistics about the value. It turns a large, sparse, binary representation of the categorical variable, such as that produced by one-hot encoding, into a very small, dense, real-valued numeric representation (Figure 5-2).\n",
    "\n",
    "n terms of implementation, bin counting requires storing a map between each category and its associated counts. (The rest of the statistics can be derived on the fly from the raw counts.) Hence it requires O(k) space, where k is the number of unique values of the categorical variable.\n",
    "\n",
    "To illustrate bin counting in practice, we’ll use data from a Kaggle competition hosted by Avazu. Here are some relevant statistics about the dataset:\n",
    "\n",
    "- There are 24 variables, including click, a binary click/no click counter, and device_id, which tracks which device an ad was displayed on.\n",
    "- The full dataset contains 40,428,967 observations, with 2,686,408 unique devices.\n",
    "\n",
    "The aim of the Avazu competition was to predict click-through rate using ad data, but we will use the dataset to demonstrate how bin counting can greatly reduce the feature space for large amounts of streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUy0n_Zbk-5H"
   },
   "source": [
    "## Bin counting Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fq8PREgCk-5I"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wADbnvOCk-5J"
   },
   "source": [
    "- [Click-through ad data from Kaggle competition](https://www.kaggle.com/c/avazu-ctr-prediction/data)\n",
    "- train_subset is first 10K rows of 6+GB set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ARVIySYk-5K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = pd.read_csv('train.csv', nrows = 10000)\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df = pd.read_csv('train.csv', nrows = 10000)\n",
    "'''\n",
    "\n",
    "# leer 'train_10k.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VXjcqvsk-5N",
    "outputId": "2cd4ba8c-c40e-4132-ddd4-00891db85ab1"
   },
   "outputs": [],
   "source": [
    "# how many features should we have after?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM3DVKuEk-5P"
   },
   "source": [
    "Features are $\\theta$ = [$N^+$, $N^-$, $log(N^+)-log(N^-)$, isRest]\n",
    "\n",
    "$N^+$ = $p(+)$ = $n^+/(n^+ + n^-)$\n",
    "\n",
    "$N^-$ = $p(-)$ = $n^-/(n^+ + n^-)$\n",
    "\n",
    "$log(N^+)-log(N^-)$ = $\\frac{p(+)}{p(-)}$\n",
    "\n",
    "isRest = back-off bin (not shown here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tJu-qhTk-5P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef click_counting(x, bin_column):\\n    clicks = pd.Series(x[x['click'] > 0][bin_column].value_counts(), name='clicks')\\n    no_clicks = pd.Series(x[x['click'] < 1][bin_column].value_counts(), name='no_clicks')\\n    \\n    counts = pd.DataFrame([clicks,no_clicks]).T.fillna('0')\\n    counts['total'] = counts['clicks'].astype('int64') + counts['no_clicks'].astype('int64')\\n    \\n    return counts\\n\\ndef bin_counting(counts):\\n    counts['N+'] = counts['clicks'].astype('int64').divide(counts['total'].astype('int64'))\\n    counts['N-'] = counts['no_clicks'].astype('int64').divide(counts['total'].astype('int64'))\\n    counts['log_N+'] = counts['N+'].divide(counts['N-'])\\n\\n#    If we wanted to only return bin-counting properties, we would filter here\\n    bin_counts = counts.filter(items= ['N+', 'N-', 'log_N+'])\\n    return counts, bin_counts\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def click_counting(x, bin_column):\n",
    "    clicks = pd.Series(x[x['click'] > 0][bin_column].value_counts(), name='clicks')\n",
    "    no_clicks = pd.Series(x[x['click'] < 1][bin_column].value_counts(), name='no_clicks')\n",
    "    \n",
    "    counts = pd.DataFrame([clicks,no_clicks]).T.fillna('0')\n",
    "    counts['total'] = counts['clicks'].astype('int64') + counts['no_clicks'].astype('int64')\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def bin_counting(counts):\n",
    "    counts['N+'] = counts['clicks'].astype('int64').divide(counts['total'].astype('int64'))\n",
    "    counts['N-'] = counts['no_clicks'].astype('int64').divide(counts['total'].astype('int64'))\n",
    "    counts['log_N+'] = counts['N+'].divide(counts['N-'])\n",
    "\n",
    "#    If we wanted to only return bin-counting properties, we would filter here\n",
    "    bin_counts = counts.filter(items= ['N+', 'N-', 'log_N+'])\n",
    "    return counts, bin_counts\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqSoKybyk-5R"
   },
   "outputs": [],
   "source": [
    "# bin counts example: device_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVoCydSDk-5T",
    "outputId": "0c812c4b-4a52-40cc-f4c7-d6e7df324b23"
   },
   "outputs": [],
   "source": [
    "# check to make sure we have all the devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sShErmckk-5W",
    "outputId": "12671f8b-4c4b-452d-b886-719c0f1b1bb9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0iOjVwlk-5Z",
    "outputId": "a3b9557c-688f-42bf-b5c9-4122382b13ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# We can see how this can change model evaluation time by comparing raw vs. bin-counting size\\nfrom sys import getsizeof\\n\\nprint('Our pandas Series, in bytes: ', getsizeof(df.filter(items= ['device_id', 'click'])))\\nprint('Our bin-counting feature, in bytes: ', getsizeof(device_bin_counts))\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# We can see how this can change model evaluation time by comparing raw vs. bin-counting size\n",
    "from sys import getsizeof\n",
    "\n",
    "print('Our pandas Series, in bytes: ', getsizeof(df.filter(items= ['device_id', 'click'])))\n",
    "print('Our bin-counting feature, in bytes: ', getsizeof(device_bin_counts))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1-CategoricalData.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
